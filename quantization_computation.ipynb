{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIs4wgVEul7R"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from quantization import Tensor\n",
    "from quantization_utils import *\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5U_Zz8QzTGwz"
   },
   "source": [
    "# Section: Quantized Mul\n",
    "\n",
    "---\n",
    "## To Begin\n",
    "\n",
    "Let's revisit our quantization schema:\n",
    "\n",
    "$$lhs_{float} = (lhs_{quant} - lhs_{zp}) * lhs_s$$\n",
    "$$rhs_{float} = (rhs_{quant} - rhs_{zp}) * rhs_s$$\n",
    "$$output_{float} = (output_{quant} - output_{zp}) * output_s$$\n",
    "\n",
    "---\n",
    "Bring those to our mul implementation:\n",
    "\n",
    "$$output_{float} = lhs_{float} * rhs_{float}$$\n",
    "    =>\n",
    "$$(output_{quant} - output_{zp}) * output_s = (lhs_{quant} - lhs_{zp}) * lhs_s * (rhs_{quant} - rhs_{zp}) * rhs_s$$\n",
    "\n",
    "=>\n",
    "$$output_{quant} - output_{zp} = (lhs_{quant} - lhs_{zp}) * (rhs_{quant} - rhs_{zp}) * (\\frac{lhs_s * rhs_s}{output_s})$$\n",
    "\n",
    "\n",
    "Let $$real_s = \\frac{lhs_s * rhs_s}{output_s}$$ since we can easily fold this one.\n",
    "\n",
    "---\n",
    "Since our goal is to compute output_q, we will have the following equation:\n",
    "\n",
    "\n",
    "$$output_{quant} = (lhs_{quant} - lhs_{zp}) * (rhs_{quant} - rhs_{zp}) * real_s + output_{zp}$$\n",
    "\n",
    "\n",
    "So far so good, but just a second, what's the promise of **integer-only**?\n",
    "Isn't $$real_s$$ a float number? \n",
    "\n",
    "---\n",
    "\n",
    "## Integer-only \n",
    "\n",
    "Let's slightly change the representation:\n",
    "\n",
    "$$real_s = real_{can} * 2^{shift}$$.\n",
    "\n",
    "$$real_{can}$$ is a canonical representation of the float value, ranges between [0.5, 1), please note our scale is always a positive value, while *shift* is essentially the shift (a left shift if positive, a right shift if negative).\n",
    "\n",
    "In fact, ```std::frexp``` will help us with the job, see the documentation [here](https://en.cppreference.com/w/cpp/numeric/math/frexp).\n",
    "\n",
    "Let's just apply one more trick:\n",
    "\n",
    "$$real_s = \\frac{real_{can} * 2^{31} * 2^{shift}}{2^{31}} $$\n",
    "\n",
    "\n",
    "and $$multiplier = real_{can} * 2^{31}$$\n",
    "\n",
    "is within the range of int32 representation, rember the canonicacal representation is within [0.5, 1)? so that value is within [2^30, 2^31 - 1], and we\n",
    "can easily cast that value to int32 with very minimum accuracy lost.\n",
    "\n",
    "---\n",
    "## Put it together\n",
    "Now we know:\n",
    "\n",
    "$$output_{quant} = \\frac{temp* multiplier}{2^{31}} * 2^{shift} + output_{zp}$$\n",
    "\n",
    "where $$temp = (lhs_{quant} - lhs_{zp}) * (rhs_{quant} - rhs_{zp})$$\n",
    "\n",
    "Here's the beautilful part:\n",
    "\n",
    "$$(*) = \\frac{temp* multiplier}{2^{31}}$$ is actually pretty *cheap* for the hardware (ARM):\n",
    "\n",
    "ARM actually has a very verbose name for that: \"Signed saturating Rounding Doubling Multiply returning High half.\" The hardware instruction is acutally just [\"VQRDMULH\"](http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0473j/dom1361289977828.html).\n",
    "\n",
    "What it does is multiplying the two values then returning the signifiant half (also, rounding).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQXnG6NbTwId"
   },
   "source": [
    "### Special Note\n",
    "In previous section, we talked about using int32 for multiplier & temp data, it's also possible to use int16 (but we need to be careful about overflow issue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZnh9u2jYLfi"
   },
   "outputs": [],
   "source": [
    "lhs_float, lhs_q_tensor = populate_data((4, 4, 4), -4.0, 5.0)\n",
    "rhs_float, rhs_q_tensor = populate_data((4, 4, 4), -1.5, 0.9)\n",
    "\n",
    "# ouput ranges (-7.5, 6.0).\n",
    "# Let's just assume the output has the following:\n",
    "out_scale = (6.0 + 7.5) / 256.0\n",
    "out_zp = -(6.0 - 7.5) / (2.0 * out_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fc_H3VnXHuY8"
   },
   "outputs": [],
   "source": [
    "def multiply(tensor1, tensor2, output_tensor, datatype=np.int32):\n",
    "  real_scale = tensor1.scale * tensor2.scale / output_tensor.scale\n",
    "  multipler_float, shift = np.frexp(real_scale)\n",
    "  if datatype == np.int32:\n",
    "    multiplier_shift = 31\n",
    "  elif datatype == np.int16:\n",
    "    multiplier_shift = 15\n",
    "  else:\n",
    "    assert False\n",
    "  multiplier = datatype(multipler_float * (2 ** multiplier_shift))\n",
    "\n",
    "  for i in range(tensor1.value.size):\n",
    "    lhs_value = datatype(tensor1.value.ravel()[i])\n",
    "    lhs_value -= tensor1.zero_point\n",
    "    rhs_value = datatype(tensor2.value.ravel()[i])\n",
    "    rhs_value -= tensor2.zero_point\n",
    "    # We need to be careful about the overflow issue when the datatype is int16\n",
    "    temp = datatype(lhs_value * rhs_value)\n",
    "    product = saturate_rounding_doubling_multiply(temp, multiplier, datatype)\n",
    "    output = apply_shift(product, shift)\n",
    "    output += output_tensor.zero_point\n",
    "    output = np.int8(min(max(output, -128), 127))\n",
    "    output_tensor.value.ravel()[i] = output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zZYEEIWJABo2",
    "outputId": "f503c4d1-e310-4f9d-b26f-45366ad64734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg deviation between lhs float and quantized representation is 0.017\n",
      "Avg deviation between rhs float and quantized representation is 0.005\n"
     ]
    }
   ],
   "source": [
    "# First, let's make sure both the inputs are close to our range.\n",
    "np.allclose(lhs_float, dequant(lhs_q_tensor), atol=5e-2)\n",
    "np.allclose(rhs_float, dequant(rhs_q_tensor), atol=5e-2)\n",
    "\n",
    "# Let's check the average deviation as well: avg(abs(a - b))\n",
    "print(\"Avg deviation between lhs float and quantized representation is %.3f\" % diff(lhs_float, dequant(lhs_q_tensor)))\n",
    "print(\"Avg deviation between rhs float and quantized representation is %.3f\" % diff(rhs_float, dequant(rhs_q_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "OM9TWOsg_1zv",
    "outputId": "4f561937-0e92-498e-d085-791143217a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With in 0.2 range  True\n",
      "Avg deviation between output float and quantized representation is 0.028\n"
     ]
    }
   ],
   "source": [
    "# True result\n",
    "expect_result = lhs_float * rhs_float\n",
    "\n",
    "\n",
    "# Let's see how the quantized kernel works out:\n",
    "out_tensor = Tensor(np.zeros(lhs_float.shape), out_scale, out_zp)\n",
    "multiply(lhs_q_tensor, rhs_q_tensor, out_tensor, np.int32)\n",
    "\n",
    "# Let's make sure it's within 0.2 range\n",
    "result = np.allclose(expect_result, dequant(out_tensor), atol=2e-1)\n",
    "print(\"With in 0.2 range \", result)\n",
    "\n",
    "# Also, let's see the average deviation:\n",
    "print(\"Avg deviation between output float and quantized representation is %.3f\" % diff(expect_result, dequant(out_tensor)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBjlE5xThDaP"
   },
   "source": [
    "## What's Next?\n",
    "*   Conv? MatMul?\n",
    "*   Fixed point?\n",
    "*   Activations?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "quantization computation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
